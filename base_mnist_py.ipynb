{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "base mnist.py",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Hoddi76/neural-network-python/blob/master/base_mnist_py.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "j76ZCn_bwwhr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential # Последовательная модель сети\n",
        "from keras.layers import Dense # Тип слоев соеденение всех нейронов пред идущего уровня со всеми следующего\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lkjDQKVxxFCz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Устанавливаем seed для повторяемости результатов\n",
        "numpy.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e8JKpafwxK0U",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Загружаем данные\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7QwUk-okxNEi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Преобразование размерности изображений\n",
        "X_train = X_train.reshape(60000, 784) # 60.000 изображений представленых одномерным масивом 784пикселя\n",
        "X_test = X_test.reshape(10000, 784)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bbfhV5WBxnHy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Нормализация данных\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sgaqUNBhxsLK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Преобразуем метки в категории\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3k4WNaQryB6Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Создаем нашу нейронную сеть**"
      ]
    },
    {
      "metadata": {
        "id": "XQUijDKAyHPp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Создаем последовательную модель\n",
        "model = Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WRTPYQKqyOu5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Добавляем уровни сети\n",
        "# 2 слоя\n",
        "model.add(Dense(800, input_dim=784, activation=\"relu\", kernel_initializer=\"normal\")) # на первом слое 800нейронов у каждого из них 784 входа(по кол-ву пикселей), веса инициализируюся случайно,функция активацию релу\n",
        "model.add(Dense(10, activation=\"softmax\", kernel_initializer=\"normal\"))  # 10 нейронов (выходной слой, у нас 10цифр 0-9), функция активации софтмакс, все инициализируются случайные"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gt5-bjovzcVf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Компилируем модель\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"]) # SGD метод обучения градиентный спуск, loss-мера ошибки, метрика точность"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d9ZuCj212NVE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "outputId": "66cfad6b-e126-422a-d40b-f979f8650a02"
      },
      "cell_type": "code",
      "source": [
        "#Печатаем характиристики\n",
        "print(model.summary())\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 800)               628000    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                8010      \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 800)               8800      \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                8010      \n",
            "=================================================================\n",
            "Total params: 652,820\n",
            "Trainable params: 652,820\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "757buYVr3Xaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4553
        },
        "outputId": "23b7752f-ef52-4eb5-dd94-f56e7967c25d"
      },
      "cell_type": "code",
      "source": [
        "# Обучаем сеть\n",
        "# парам1 = данные обучения, парам2 = правильные ответы, размер мини выборки 200элементов,эпохи,verbose позволяет печатать диагностическую инфо во время обучения.\n",
        "model.fit(X_train, Y_train, batch_size=200, epochs=125, validation_split=0.2, verbose=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.8547 - acc: 0.6989 - val_loss: 0.9023 - val_acc: 0.6606\n",
            "Epoch 2/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.8399 - acc: 0.7025 - val_loss: 0.8727 - val_acc: 0.6778\n",
            "Epoch 3/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.8264 - acc: 0.7028 - val_loss: 0.8188 - val_acc: 0.7334\n",
            "Epoch 4/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.8020 - acc: 0.7237 - val_loss: 0.8009 - val_acc: 0.7197\n",
            "Epoch 5/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.7937 - acc: 0.7172 - val_loss: 0.8055 - val_acc: 0.7132\n",
            "Epoch 6/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.7779 - acc: 0.7265 - val_loss: 0.7775 - val_acc: 0.7285\n",
            "Epoch 7/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.7604 - acc: 0.7353 - val_loss: 0.7834 - val_acc: 0.7227\n",
            "Epoch 8/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.7541 - acc: 0.7375 - val_loss: 0.7815 - val_acc: 0.7154\n",
            "Epoch 9/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.7404 - acc: 0.7414 - val_loss: 0.7422 - val_acc: 0.7380\n",
            "Epoch 10/125\n",
            "48000/48000 [==============================] - 2s 37us/step - loss: 0.7219 - acc: 0.7533 - val_loss: 0.7369 - val_acc: 0.7482\n",
            "Epoch 11/125\n",
            "48000/48000 [==============================] - 2s 37us/step - loss: 0.7129 - acc: 0.7560 - val_loss: 0.7785 - val_acc: 0.7088\n",
            "Epoch 12/125\n",
            "48000/48000 [==============================] - 2s 37us/step - loss: 0.7091 - acc: 0.7544 - val_loss: 0.7124 - val_acc: 0.7664\n",
            "Epoch 13/125\n",
            "48000/48000 [==============================] - 2s 37us/step - loss: 0.6922 - acc: 0.7680 - val_loss: 0.7910 - val_acc: 0.6706\n",
            "Epoch 14/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.6877 - acc: 0.7692 - val_loss: 0.6967 - val_acc: 0.7711\n",
            "Epoch 15/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.6736 - acc: 0.7776 - val_loss: 0.6891 - val_acc: 0.7988\n",
            "Epoch 16/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.6631 - acc: 0.7870 - val_loss: 0.6967 - val_acc: 0.7835\n",
            "Epoch 17/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.6590 - acc: 0.7900 - val_loss: 0.6718 - val_acc: 0.7917\n",
            "Epoch 18/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.6456 - acc: 0.7995 - val_loss: 0.7236 - val_acc: 0.7392\n",
            "Epoch 19/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.6391 - acc: 0.8060 - val_loss: 0.7039 - val_acc: 0.7446\n",
            "Epoch 20/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.6209 - acc: 0.8189 - val_loss: 0.6490 - val_acc: 0.8167\n",
            "Epoch 21/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.6147 - acc: 0.8216 - val_loss: 0.6364 - val_acc: 0.8239\n",
            "Epoch 22/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.6019 - acc: 0.8296 - val_loss: 0.6344 - val_acc: 0.8263\n",
            "Epoch 23/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.5876 - acc: 0.8371 - val_loss: 0.6175 - val_acc: 0.8357\n",
            "Epoch 24/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.5757 - acc: 0.8440 - val_loss: 0.6102 - val_acc: 0.8348\n",
            "Epoch 25/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.5649 - acc: 0.8488 - val_loss: 0.5971 - val_acc: 0.8412\n",
            "Epoch 26/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.5575 - acc: 0.8489 - val_loss: 0.5942 - val_acc: 0.8406\n",
            "Epoch 27/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.5455 - acc: 0.8539 - val_loss: 0.5776 - val_acc: 0.8476\n",
            "Epoch 28/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.5269 - acc: 0.8643 - val_loss: 0.5677 - val_acc: 0.8526\n",
            "Epoch 29/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.5141 - acc: 0.8671 - val_loss: 0.5612 - val_acc: 0.8534\n",
            "Epoch 30/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.5048 - acc: 0.8707 - val_loss: 0.5593 - val_acc: 0.8451\n",
            "Epoch 31/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.5009 - acc: 0.8677 - val_loss: 0.5445 - val_acc: 0.8598\n",
            "Epoch 32/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.4849 - acc: 0.8755 - val_loss: 0.7162 - val_acc: 0.7008\n",
            "Epoch 33/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4748 - acc: 0.8814 - val_loss: 0.5470 - val_acc: 0.8485\n",
            "Epoch 34/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4671 - acc: 0.8821 - val_loss: 0.5595 - val_acc: 0.8373\n",
            "Epoch 35/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.4496 - acc: 0.8942 - val_loss: 0.5173 - val_acc: 0.8651\n",
            "Epoch 36/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.4541 - acc: 0.8840 - val_loss: 0.5245 - val_acc: 0.8593\n",
            "Epoch 37/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.4345 - acc: 0.8983 - val_loss: 0.5016 - val_acc: 0.8755\n",
            "Epoch 38/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.4331 - acc: 0.8950 - val_loss: 0.5060 - val_acc: 0.8655\n",
            "Epoch 39/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.4217 - acc: 0.8998 - val_loss: 0.5312 - val_acc: 0.8411\n",
            "Epoch 40/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.4118 - acc: 0.9037 - val_loss: 0.5227 - val_acc: 0.8559\n",
            "Epoch 41/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4064 - acc: 0.9050 - val_loss: 0.5051 - val_acc: 0.8557\n",
            "Epoch 42/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.4003 - acc: 0.9064 - val_loss: 0.5122 - val_acc: 0.8584\n",
            "Epoch 43/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3901 - acc: 0.9084 - val_loss: 0.4653 - val_acc: 0.8877\n",
            "Epoch 44/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3788 - acc: 0.9171 - val_loss: 0.4650 - val_acc: 0.8875\n",
            "Epoch 45/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3702 - acc: 0.9199 - val_loss: 0.4768 - val_acc: 0.8723\n",
            "Epoch 46/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3638 - acc: 0.9214 - val_loss: 0.4617 - val_acc: 0.8859\n",
            "Epoch 47/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3570 - acc: 0.9219 - val_loss: 0.4595 - val_acc: 0.8878\n",
            "Epoch 48/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3421 - acc: 0.9301 - val_loss: 0.4744 - val_acc: 0.8774\n",
            "Epoch 49/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3348 - acc: 0.9314 - val_loss: 0.4737 - val_acc: 0.8710\n",
            "Epoch 50/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3261 - acc: 0.9332 - val_loss: 0.4806 - val_acc: 0.8746\n",
            "Epoch 51/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.3177 - acc: 0.9359 - val_loss: 0.4234 - val_acc: 0.9037\n",
            "Epoch 52/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3098 - acc: 0.9382 - val_loss: 0.4133 - val_acc: 0.9067\n",
            "Epoch 53/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.3000 - acc: 0.9407 - val_loss: 0.4569 - val_acc: 0.8857\n",
            "Epoch 54/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2890 - acc: 0.9447 - val_loss: 0.4027 - val_acc: 0.9097\n",
            "Epoch 55/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2836 - acc: 0.9456 - val_loss: 0.4012 - val_acc: 0.9080\n",
            "Epoch 56/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2802 - acc: 0.9440 - val_loss: 0.4001 - val_acc: 0.9095\n",
            "Epoch 57/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2707 - acc: 0.9479 - val_loss: 0.4304 - val_acc: 0.8921\n",
            "Epoch 58/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2571 - acc: 0.9521 - val_loss: 0.4039 - val_acc: 0.9063\n",
            "Epoch 59/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2547 - acc: 0.9516 - val_loss: 0.3838 - val_acc: 0.9135\n",
            "Epoch 60/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2662 - acc: 0.9449 - val_loss: 0.3803 - val_acc: 0.9159\n",
            "Epoch 61/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2387 - acc: 0.9559 - val_loss: 0.3848 - val_acc: 0.9109\n",
            "Epoch 62/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2325 - acc: 0.9577 - val_loss: 0.3722 - val_acc: 0.9134\n",
            "Epoch 63/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2482 - acc: 0.9479 - val_loss: 0.3689 - val_acc: 0.9162\n",
            "Epoch 64/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2195 - acc: 0.9605 - val_loss: 0.3638 - val_acc: 0.9172\n",
            "Epoch 65/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2172 - acc: 0.9592 - val_loss: 0.3770 - val_acc: 0.9119\n",
            "Epoch 66/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2398 - acc: 0.9517 - val_loss: 0.3649 - val_acc: 0.9178\n",
            "Epoch 67/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.2044 - acc: 0.9631 - val_loss: 0.3602 - val_acc: 0.9178\n",
            "Epoch 68/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1994 - acc: 0.9645 - val_loss: 0.3880 - val_acc: 0.9060\n",
            "Epoch 69/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1961 - acc: 0.9656 - val_loss: 0.3629 - val_acc: 0.9149\n",
            "Epoch 70/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1921 - acc: 0.9655 - val_loss: 0.3542 - val_acc: 0.9180\n",
            "Epoch 71/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2138 - acc: 0.9568 - val_loss: 0.3504 - val_acc: 0.9207\n",
            "Epoch 72/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1816 - acc: 0.9678 - val_loss: 0.3562 - val_acc: 0.9172\n",
            "Epoch 73/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1773 - acc: 0.9690 - val_loss: 0.3531 - val_acc: 0.9193\n",
            "Epoch 74/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1753 - acc: 0.9695 - val_loss: 0.3506 - val_acc: 0.9193\n",
            "Epoch 75/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1974 - acc: 0.9619 - val_loss: 0.3478 - val_acc: 0.9207\n",
            "Epoch 76/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1676 - acc: 0.9709 - val_loss: 0.3479 - val_acc: 0.9188\n",
            "Epoch 77/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1869 - acc: 0.9626 - val_loss: 0.3459 - val_acc: 0.9216\n",
            "Epoch 78/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1749 - acc: 0.9665 - val_loss: 0.3451 - val_acc: 0.9198\n",
            "Epoch 79/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1556 - acc: 0.9739 - val_loss: 0.3463 - val_acc: 0.9202\n",
            "Epoch 80/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1523 - acc: 0.9743 - val_loss: 0.3445 - val_acc: 0.9200\n",
            "Epoch 81/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1482 - acc: 0.9753 - val_loss: 0.3643 - val_acc: 0.9128\n",
            "Epoch 82/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1480 - acc: 0.9749 - val_loss: 0.3366 - val_acc: 0.9219\n",
            "Epoch 83/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1703 - acc: 0.9652 - val_loss: 0.3346 - val_acc: 0.9233\n",
            "Epoch 84/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1387 - acc: 0.9775 - val_loss: 0.3443 - val_acc: 0.9201\n",
            "Epoch 85/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1354 - acc: 0.9781 - val_loss: 0.3910 - val_acc: 0.9060\n",
            "Epoch 86/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1588 - acc: 0.9699 - val_loss: 0.3382 - val_acc: 0.9230\n",
            "Epoch 87/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1303 - acc: 0.9787 - val_loss: 0.3397 - val_acc: 0.9204\n",
            "Epoch 88/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1272 - acc: 0.9802 - val_loss: 0.3381 - val_acc: 0.9227\n",
            "Epoch 89/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1419 - acc: 0.9747 - val_loss: 1.0816 - val_acc: 0.6310\n",
            "Epoch 90/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1528 - acc: 0.9699 - val_loss: 0.3464 - val_acc: 0.9195\n",
            "Epoch 91/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1211 - acc: 0.9808 - val_loss: 0.3375 - val_acc: 0.9211\n",
            "Epoch 92/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1167 - acc: 0.9825 - val_loss: 0.3305 - val_acc: 0.9230\n",
            "Epoch 93/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1166 - acc: 0.9821 - val_loss: 0.3302 - val_acc: 0.9244\n",
            "Epoch 94/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1131 - acc: 0.9825 - val_loss: 0.3398 - val_acc: 0.9197\n",
            "Epoch 95/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1091 - acc: 0.9845 - val_loss: 0.3327 - val_acc: 0.9232\n",
            "Epoch 96/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1738 - acc: 0.9637 - val_loss: 0.3308 - val_acc: 0.9249\n",
            "Epoch 97/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1064 - acc: 0.9841 - val_loss: 0.3309 - val_acc: 0.9244\n",
            "Epoch 98/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1033 - acc: 0.9851 - val_loss: 0.3363 - val_acc: 0.9227\n",
            "Epoch 99/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.1016 - acc: 0.9860 - val_loss: 0.3319 - val_acc: 0.9242\n",
            "Epoch 100/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0982 - acc: 0.9866 - val_loss: 0.3299 - val_acc: 0.9231\n",
            "Epoch 101/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0964 - acc: 0.9871 - val_loss: 0.3361 - val_acc: 0.9222\n",
            "Epoch 102/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0951 - acc: 0.9871 - val_loss: 0.3378 - val_acc: 0.9214\n",
            "Epoch 103/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.1535 - acc: 0.9689 - val_loss: 0.3275 - val_acc: 0.9264\n",
            "Epoch 104/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0929 - acc: 0.9873 - val_loss: 0.3472 - val_acc: 0.9167\n",
            "Epoch 105/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0896 - acc: 0.9882 - val_loss: 0.3323 - val_acc: 0.9233\n",
            "Epoch 106/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0879 - acc: 0.9888 - val_loss: 0.3292 - val_acc: 0.9247\n",
            "Epoch 107/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0856 - acc: 0.9894 - val_loss: 0.3331 - val_acc: 0.9232\n",
            "Epoch 108/125\n",
            "48000/48000 [==============================] - 2s 37us/step - loss: 0.0850 - acc: 0.9896 - val_loss: 0.3279 - val_acc: 0.9243\n",
            "Epoch 109/125\n",
            "48000/48000 [==============================] - 2s 37us/step - loss: 0.0815 - acc: 0.9900 - val_loss: 0.3332 - val_acc: 0.9237\n",
            "Epoch 110/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0807 - acc: 0.9908 - val_loss: 0.3403 - val_acc: 0.9202\n",
            "Epoch 111/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0788 - acc: 0.9910 - val_loss: 0.3298 - val_acc: 0.9245\n",
            "Epoch 112/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0779 - acc: 0.9913 - val_loss: 0.3351 - val_acc: 0.9225\n",
            "Epoch 113/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0762 - acc: 0.9913 - val_loss: 0.3341 - val_acc: 0.9229\n",
            "Epoch 114/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0755 - acc: 0.9916 - val_loss: 0.3281 - val_acc: 0.9239\n",
            "Epoch 115/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0731 - acc: 0.9919 - val_loss: 0.3300 - val_acc: 0.9242\n",
            "Epoch 116/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0716 - acc: 0.9928 - val_loss: 0.3297 - val_acc: 0.9233\n",
            "Epoch 117/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0712 - acc: 0.9926 - val_loss: 0.3314 - val_acc: 0.9238\n",
            "Epoch 118/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0698 - acc: 0.9923 - val_loss: 0.3338 - val_acc: 0.9233\n",
            "Epoch 119/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0693 - acc: 0.9925 - val_loss: 0.3325 - val_acc: 0.9222\n",
            "Epoch 120/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0654 - acc: 0.9936 - val_loss: 0.3342 - val_acc: 0.9230\n",
            "Epoch 121/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0653 - acc: 0.9936 - val_loss: 0.3432 - val_acc: 0.9210\n",
            "Epoch 122/125\n",
            "48000/48000 [==============================] - 2s 36us/step - loss: 0.0637 - acc: 0.9938 - val_loss: 0.3373 - val_acc: 0.9221\n",
            "Epoch 123/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0624 - acc: 0.9940 - val_loss: 0.3314 - val_acc: 0.9233\n",
            "Epoch 124/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.2067 - acc: 0.9579 - val_loss: 0.3376 - val_acc: 0.9265\n",
            "Epoch 125/125\n",
            "48000/48000 [==============================] - 2s 35us/step - loss: 0.0697 - acc: 0.9912 - val_loss: 0.3354 - val_acc: 0.9253\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd66d72a470>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "23tTXjJW5Xa6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3e0fbf06-3f56-46a9-c7eb-0333073c97fa"
      },
      "cell_type": "code",
      "source": [
        "# Оцениваем качество обучения сети на тестовых данных\n",
        "scores = model.evaluate(X_test, Y_test, verbose=0)\n",
        "print(\"Точность работы на тестовых данных: %.2f%%\" % (scores[1]*100))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Точность работы на тестовых данных: 92.10%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}